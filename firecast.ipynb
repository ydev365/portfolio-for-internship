{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ydev365/portfolio-for-internship/blob/main/firecast.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sb-YjTvmyNVc",
        "outputId": "a1cee3b3-e0a3-4ac8-b2c5-8b5ce4a4970c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0Yx0tnWfEn5"
      },
      "source": [
        "이 코드는 그냥 8명 단체단톡방인 FIRECAST에 올려둔 call119_train1 로 돌려봤는데 성능이 RSME 가 1.6 이다. RSME 라는 숫자가 낮을수록 성능이 좋은거다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KHE495jXxGMZ",
        "outputId": "602edeac-f951-4d7d-cda7-f28d11a48739"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-9-2129252055>:52: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
            "  X = X.fillna(method='ffill')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "검증 데이터의 RMSE: 1.6767286110690767\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/call119_train1.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# --- Start of modifications for handling missing values using ffill ---\n",
        "\n",
        "# List of columns identified to have -99.0 as a placeholder for missing values\n",
        "# Based on the previous analysis of the CSV file.\n",
        "columns_to_impute = [\n",
        "    'call119_train.ta_max',\n",
        "    'call119_train.ta_min',\n",
        "    'call119_train.ta_max_min',\n",
        "    'call119_train.hm_min',\n",
        "    'call119_train.hm_max',\n",
        "    'call119_train.ws_max',\n",
        "    'call119_train.ws_ins_max',\n",
        "    'call119_train.rn_day'\n",
        "]\n",
        "\n",
        "# Replace -99.0 with NaN in the identified columns\n",
        "for col in columns_to_impute:\n",
        "    df[col] = df[col].replace(-99.0, np.nan)\n",
        "\n",
        "# --- End of modifications for handling missing values ---\n",
        "\n",
        "# Separate target variable Y from features X\n",
        "X = df.drop('call119_train.call_count', axis=1)\n",
        "y = df['call119_train.call_count']\n",
        "\n",
        "# Drop 'call119_train.address_city' column as specified\n",
        "X = X.drop('call119_train.address_city', axis=1)\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder to 'call119_train.address_gu' and 'call119_train.sub_address'\n",
        "# These are handled before imputation because they are categorical and ffill is for numerical.\n",
        "X['call119_train.address_gu'] = label_encoder.fit_transform(X['call119_train.address_gu'])\n",
        "X['call119_train.sub_address'] = label_encoder.fit_transform(X['call119_train.sub_address'])\n",
        "\n",
        "# --- New step: Impute missing values with forward fill (ffill) ---\n",
        "# Apply ffill to all numerical columns that might contain NaNs after placeholder replacement.\n",
        "# Note: If there are NaNs at the very beginning of a column, ffill will not fill them.\n",
        "# In such cases, a bfill (backward fill) or mean/median imputation might be needed as a fallback.\n",
        "X = X.fillna(method='ffill')\n",
        "\n",
        "# Optional: If there are still NaNs after ffill (e.g., if the first value was NaN),\n",
        "# you might want to fill remaining NaNs with the mean or 0 as a last resort.\n",
        "# X = X.fillna(X.mean(numeric_only=True))\n",
        "\n",
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize and train the RandomForestRegressor model\n",
        "model = RandomForestRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_pred = model.predict(X_val)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
        "\n",
        "print(f'검증 데이터의 RMSE: {rmse}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQmzR8YOfi8Z"
      },
      "source": [
        "이거는 이제 제가 전처리한 파일인 preprocessed_full_data_V3.csv 저희 4명 팀즈 채팅방에 올려놓은 걸로 돌렸더니 RSME 가 1.48로 개선됐다. 전처리를 잘해야 예측값이 더 정확해지는구나. 하지만 나중가서 안사실은 결측치를 비워두고 모델이 알아서 처리하게끔해야 성능이 더 잘나오는 것도 있다. 사람이 결측치를 처리하는것보다 기계가 결측치를 처리할때 성능이 더 잘뽑힐때가 많다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-vss6o-bFPeu",
        "outputId": "3bcd98d1-bd9b-4924-ce38-e579c6b7ceea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE: 1.4809474772084894\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from scipy.stats import randint\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/preprocessed_full_data_V3.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract year from 'call119_train.tm'\n",
        "# Assuming 'call119_train.tm' is an integer like 2020010100\n",
        "df['year'] = df['call119_train.tm'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])]\n",
        "test_df = df[df['year'] == 2023]\n",
        "\n",
        "# Define features (X) and target (Y)\n",
        "# Exclude 'Unnamed: 0', 'year', and the target 'call119_train.call_count' from features\n",
        "features = [col for col in df.columns if col not in ['Unnamed: 0', 'year', 'call119_train.call_count']]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "X_train = train_df[features]\n",
        "X_train=X_train.drop('call119_train.tm', axis=1)\n",
        "Y_train = train_df[target]\n",
        "\n",
        "X_test = test_df[features]\n",
        "X_test=X_test.drop('call119_train.tm', axis=1)\n",
        "Y_test = test_df[target]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Initialize and train RandomForestRegressor model\n",
        "# Set random_state for reproducibility\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "\n",
        "print(f\"RMSE: {rmse}\")\n",
        "\n",
        "#RMSE: 1.4809474772084894"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNSWBh6Ff_OY"
      },
      "source": [
        "이 코드는 제가 월,일 날짜 데이터를 넣은채 돌렸더니 성능이 약간 안좋아졌다. 그냥 날짜데이터를 아예 뺀게 애초에 더 좋은 성능을 나타냈다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6bLPHWyOtnA",
        "outputId": "c32080a2-21b8-4397-da03-1509606d518a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RMSE after modifying 'call119_train.tm': 1.49491462326673\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import numpy as np\n",
        "\n",
        "# Load the dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/preprocessed_full_data_V3.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Extract year from 'call119_train.tm' for splitting\n",
        "# Assuming 'call119_train.tm' is an integer like 2020010100\n",
        "df['year'] = df['call119_train.tm'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])].copy() # .copy() to avoid SettingWithCopyWarning\n",
        "test_df = df[df['year'] == 2023].copy() # .copy() to avoid SettingWithCopyWarning\n",
        "\n",
        "# Define features (X) and target (Y)\n",
        "# Exclude 'Unnamed: 0', 'year', and the target 'call119_train.call_count' from features\n",
        "features = [col for col in df.columns if col not in ['Unnamed: 0', 'year', 'call119_train.call_count']]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "X_train = train_df[features].copy()\n",
        "Y_train = train_df[target].copy()\n",
        "X_test = test_df[features].copy()\n",
        "Y_test = test_df[target].copy()\n",
        "\n",
        "# Modify 'call119_train.tm' in X_train and X_test\n",
        "# Convert to string, remove the '.0', then take the last 4 digits (MMDD)\n",
        "X_train['call119_train.tm'] = X_train['call119_train.tm'].astype(str).str.replace('.0', '', regex=False).str[-4:].astype(int)\n",
        "X_test['call119_train.tm'] = X_test['call119_train.tm'].astype(str).str.replace('.0', '', regex=False).str[-4:].astype(int)\n",
        "\n",
        "# Initialize and train RandomForestRegressor model\n",
        "# Set random_state for reproducibility\n",
        "rf_model = RandomForestRegressor(random_state=42)\n",
        "rf_model.fit(X_train, Y_train)\n",
        "\n",
        "# Make predictions on the test set\n",
        "Y_pred = rf_model.predict(X_test)\n",
        "\n",
        "# Calculate RMSE\n",
        "rmse = np.sqrt(mean_squared_error(Y_test, Y_pred))\n",
        "\n",
        "print(f\"RMSE after modifying 'call119_train.tm': {rmse}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eozk8GYahKKn"
      },
      "source": [
        "XGboost 하이퍼파라미터 조정하니까 이것도 RSME 1.26으로 괜찮게 나왔다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_B4cuCsaF0M",
        "outputId": "315c13c1-6912-441f-d433-9bf4c2a828a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best XGB parameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 300}\n",
            "XGBRegressor Tuned RMSE: 1.2617998126653973\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "# XGBoost 임포트\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # 재사용\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/preprocessed_full_data_V3.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract year from 'call119_train.tm'\n",
        "df['year'] = df['call119_train.tm'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])].copy()\n",
        "test_df = df[df['year'] == 2023].copy()\n",
        "\n",
        "# Define features (X) and target (Y)\n",
        "features = [col for col in df.columns if col not in ['Unnamed: 0', 'year', 'call119_train.call_count']]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "X_train = train_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_train = train_df[target]\n",
        "\n",
        "X_test = test_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_test = test_df[target]\n",
        "\n",
        "# --- RandomForestRegressor 결과 ---\n",
        "# rf_model = RandomForestRegressor(random_state=42)\n",
        "# rf_model.fit(X_train, Y_train)\n",
        "# Y_pred_rf = rf_model.predict(X_test)\n",
        "# rmse_rf = np.sqrt(mean_squared_error(Y_test, Y_pred_rf))\n",
        "# print(f\"RandomForestRegressor RMSE: {rmse_rf}\")\n",
        "\n",
        "# --- LightGBM 결과 ---\n",
        "# lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "# lgb_model.fit(X_train, Y_train)\n",
        "# Y_pred_lgb = lgb_model.predict(X_test)\n",
        "# rmse_lgb = np.sqrt(mean_squared_error(Y_test, Y_pred_lgb))\n",
        "# print(f\"LGBMRegressor RMSE: {rmse_lgb}\")\n",
        "\n",
        "\n",
        "# --- XGBoost 모델 ---\n",
        "# print(\"\\n--- XGBoost (XGBRegressor) ---\")\n",
        "# # objective='reg:squarederror'는 회귀 문제에서 제곱 오차를 최소화하도록 지정\n",
        "# xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "# xgb_model.fit(X_train, Y_train)\n",
        "\n",
        "# Y_pred_xgb = xgb_model.predict(X_test)\n",
        "# rmse_xgb = np.sqrt(mean_squared_error(Y_test, Y_pred_xgb))\n",
        "# print(f\"XGBRegressor RMSE: {rmse_xgb}\")\n",
        "\n",
        "# XGBoost 하이퍼파라미터 튜닝 예시 (간략히)\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7]\n",
        "}\n",
        "grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_xgb.fit(X_train, Y_train)\n",
        "print(f\"Best XGB parameters: {grid_search_xgb.best_params_}\")\n",
        "Y_pred_xgb_tuned = grid_search_xgb.best_estimator_.predict(X_test)\n",
        "rmse_xgb_tuned = np.sqrt(mean_squared_error(Y_test, Y_pred_xgb_tuned))\n",
        "print(f\"XGBRegressor Tuned RMSE: {rmse_xgb_tuned}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g60ucVWAg3jo"
      },
      "source": [
        "이 밑의 코드는 하이퍼파라미터를 조정하니까 성능이 살짝 구리게 나왔다. RSME 가 1.24로 나왔다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wEhIuIgZcwS",
        "outputId": "f76560d3-a0c2-4fb4-bd16-2f1c0431f723"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001015 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1922\n",
            "[LightGBM] [Info] Number of data points in the train set: 32372, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 1.950358\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Best LGBM parameters: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200}\n",
            "LGBMRegressor Tuned RMSE: 1.2492637927738748\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# LightGBM 임포트\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # 재사용\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/preprocessed_full_data_V3.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract year from 'call119_train.tm'\n",
        "df['year'] = df['call119_train.tm'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])].copy()\n",
        "test_df = df[df['year'] == 2023].copy()\n",
        "\n",
        "# Define features (X) and target (Y)\n",
        "features = [col for col in df.columns if col not in ['Unnamed: 0', 'year', 'call119_train.call_count']]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "X_train = train_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_train = train_df[target]\n",
        "\n",
        "X_test = test_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_test = test_df[target]\n",
        "\n",
        "# --- RandomForestRegressor 결과 (이전 코드) ---\n",
        "# rf_model = RandomForestRegressor(random_state=42)\n",
        "# rf_model.fit(X_train, Y_train)\n",
        "# Y_pred_rf = rf_model.predict(X_test)\n",
        "# rmse_rf = np.sqrt(mean_squared_error(Y_test, Y_pred_rf))\n",
        "# print(f\"RandomForestRegressor RMSE: {rmse_rf}\")\n",
        "\n",
        "\n",
        "# --- LightGBM 모델 ---\n",
        "# print(\"\\n--- LightGBM (LGBMRegressor) ---\")\n",
        "# lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "# lgb_model.fit(X_train, Y_train)\n",
        "\n",
        "# Y_pred_lgb = lgb_model.predict(X_test)\n",
        "# rmse_lgb = np.sqrt(mean_squared_error(Y_test, Y_pred_lgb))\n",
        "# print(f\"LGBMRegressor RMSE: {rmse_lgb}\")\n",
        "\n",
        "# LightGBM 하이퍼파라미터 튜닝 예시 (간략히)\n",
        "param_grid_lgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [5, 10, 15]\n",
        "}\n",
        "grid_search_lgb = GridSearchCV(lgb_model, param_grid_lgb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "grid_search_lgb.fit(X_train, Y_train)\n",
        "print(f\"Best LGBM parameters: {grid_search_lgb.best_params_}\")\n",
        "Y_pred_lgb_tuned = grid_search_lgb.best_estimator_.predict(X_test)\n",
        "rmse_lgb_tuned = np.sqrt(mean_squared_error(Y_test, Y_pred_lgb_tuned))\n",
        "print(f\"LGBMRegressor Tuned RMSE: {rmse_lgb_tuned}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "139DI0bee9-q"
      },
      "source": [
        " RSME 1.23로 나옴\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjLYtnoebPdY",
        "outputId": "06b8c60a-24ec-4700-dbe7-b054cc15d628"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001053 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1922\n",
            "[LightGBM] [Info] Number of data points in the train set: 32372, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 1.950358\n",
            "LGBMRegressor RMSE: 1.2333911206517731\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "# XGBoost 임포트\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # 재사용\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/preprocessed_full_data_V3.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Extract year from 'call119_train.tm'\n",
        "df['year'] = df['call119_train.tm'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])].copy()\n",
        "test_df = df[df['year'] == 2023].copy()\n",
        "\n",
        "# Define features (X) and target (Y)\n",
        "features = [col for col in df.columns if col not in ['Unnamed: 0', 'year', 'call119_train.call_count']]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "X_train = train_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_train = train_df[target]\n",
        "\n",
        "X_test = test_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_test = test_df[target]\n",
        "\n",
        "# --- RandomForestRegressor 결과 ---\n",
        "# rf_model = RandomForestRegressor(random_state=42)\n",
        "# rf_model.fit(X_train, Y_train)\n",
        "# Y_pred_rf = rf_model.predict(X_test)\n",
        "# rmse_rf = np.sqrt(mean_squared_error(Y_test, Y_pred_rf))\n",
        "# print(f\"RandomForestRegressor RMSE: {rmse_rf}\")\n",
        "\n",
        "# --- LightGBM 결과 ---\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "lgb_model.fit(X_train, Y_train)\n",
        "Y_pred_lgb = lgb_model.predict(X_test)\n",
        "rmse_lgb = np.sqrt(mean_squared_error(Y_test, Y_pred_lgb))\n",
        "print(f\"LGBMRegressor RMSE: {rmse_lgb}\")\n",
        "\n",
        "\n",
        "# --- XGBoost 모델 ---\n",
        "# print(\"\\n--- XGBoost (XGBRegressor) ---\")\n",
        "# # objective='reg:squarederror'는 회귀 문제에서 제곱 오차를 최소화하도록 지정\n",
        "# xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
        "# xgb_model.fit(X_train, Y_train)\n",
        "\n",
        "# Y_pred_xgb = xgb_model.predict(X_test)\n",
        "# rmse_xgb = np.sqrt(mean_squared_error(Y_test, Y_pred_xgb))\n",
        "# print(f\"XGBRegressor RMSE: {rmse_xgb}\")\n",
        "\n",
        "# XGBoost 하이퍼파라미터 튜닝 예시 (간략히)\n",
        "# param_grid_xgb = {\n",
        "#     'n_estimators': [100, 200, 300],\n",
        "#     'learning_rate': [0.01, 0.05, 0.1],\n",
        "#     'max_depth': [3, 5, 7]\n",
        "# }\n",
        "# grid_search_xgb = GridSearchCV(xgb_model, param_grid_xgb, cv=3, scoring='neg_mean_squared_error', n_jobs=-1)\n",
        "# grid_search_xgb.fit(X_train, Y_train)\n",
        "# print(f\"Best XGB parameters: {grid_search_xgb.best_params_}\")\n",
        "# Y_pred_xgb_tuned = grid_search_xgb.best_estimator_.predict(X_test)\n",
        "# rmse_xgb_tuned = np.sqrt(mean_squared_error(Y_test, Y_pred_xgb_tuned))\n",
        "# print(f\"XGBRegressor Tuned RMSE: {rmse_xgb_tuned}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gRy9MAdARZ_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "범주형 칼럼을 인코딩 할 필요없이 칼럼의 object type을 category로 바꾸니까 성능이 크게 향상됐다. RSME 1.23=> 1.16 으로\n"
      ],
      "metadata": {
        "id": "uYvING9aRjQi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "# XGBoost 임포트\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # 재사용\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/call119_train1.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.replace(-99, np.nan, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "# Extract year from 'call119_train.tm'\n",
        "df['year'] = df['call119_train.tm'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])].copy()\n",
        "test_df = df[df['year'] == 2023].copy()\n",
        "\n",
        "\n",
        "# Define features (X) and target (Y)\n",
        "features = [col for col in df.columns if col not in ['Unnamed: 0', 'year', 'call119_train.call_count','call119_train.address_city','call119_train.stn']]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "\n",
        "\n",
        "X_train = train_df[features].drop('call119_train.tm', axis=1)\n",
        "\n",
        "\n",
        "#odject=> category 로 바꿔야 알아서 범주형 칼럼을 인코딩해준다.\n",
        "for col in X_train.select_dtypes(include='object').columns:\n",
        "    X_train[col] = X_train[col].astype('category')\n",
        "\n",
        "Y_train = train_df[target]\n",
        "\n",
        "#odject=> category 로 바꿔야 알아서 범주형 칼럼을 인코딩해준다.\n",
        "X_test = test_df[features].drop('call119_train.tm', axis=1)\n",
        "\n",
        "for coll in X_test.select_dtypes(include='object').columns:\n",
        "    X_test[coll] = X_test[coll].astype('category')\n",
        "\n",
        "Y_test = test_df[target]\n",
        "\n",
        "\n",
        "# # --- LightGBM 결과 ---\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "lgb_model.fit(X_train, Y_train)\n",
        "Y_pred_lgb = lgb_model.predict(X_test)\n",
        "rmse_lgb = np.sqrt(mean_squared_error(Y_test, Y_pred_lgb))\n",
        "print(f\"LGBMRegressor RMSE: {rmse_lgb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-MZ5eZwNT3m",
        "outputId": "17ada846-2fc2-420e-f51a-d8f6085a8294"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003344 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1845\n",
            "[LightGBM] [Info] Number of data points in the train set: 32372, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 1.950358\n",
            "LGBMRegressor RMSE: 1.1664874808829957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "날짜 데이터를 아예 빼지말고 연도만 추출해서 넣으니까 RSME가 1.16=>1.15로 성능이 좋아짐. 단일 모델 데이터로는 이것이 최고로 나왔다. 연도를 넣으니까 성능이 더 향상되는 이유는 내 갠적 생각에는 코로나 때문인것 같다. 코로나는 2019부터 시작해서 2022년까지 우릴 괴롭히고 구급대원을 불러 신고한 사람들이 많았으니 이에따른 신고 증가 그리고 연도가 올라갈수록 코로나가 잠잠해져 뚜렷한 신고 감소가 나타났으므로, 각종 온도, 습도, 풍속 등의 날씨 데이터만으로는 신고의 추이가 반영이 안된것이 연도 정보와 함께 넣으니까 신고의 수가 반영된것같다."
      ],
      "metadata": {
        "id": "GGq2Y5AcZmsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "import lightgbm as lgb\n",
        "# XGBoost 임포트\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # 재사용\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/call119_train1.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.replace(-99, np.nan, inplace=True)\n",
        "\n",
        "# --- 1. 날짜 데이터 변환 및 특성 공학 ---\n",
        "print(\"날짜 특성을 생성합니다...\")\n",
        "# 'call119_train.tm'을 datetime 객체로 변환합니다. (YYYYMMDD 형식이라고 가정)\n",
        "df['call119_train.tm'] = pd.to_datetime(df['call119_train.tm'], format='%Y%m%d')\n",
        "\n",
        "# 기본 날짜 특성 추출\n",
        "df['year'] = df['call119_train.tm'].dt.year\n",
        "df['month'] = df['call119_train.tm'].dt.month\n",
        "df['day'] = df['call119_train.tm'].dt.day\n",
        "# df['day_of_week'] = df['call119_train.tm'].dt.dayofweek  # 월요일=0, 일요일=6\n",
        "# df['day_of_year'] = df['call119_train.tm'].dt.dayofyear\n",
        "# df['week_of_year'] = df['call119_train.tm'].dt.isocalendar().week.astype(int)\n",
        "# df['quarter'] = df['call119_train.tm'].dt.quarter\n",
        "\n",
        "# 주기적 특성을 위한 Sine/Cosine 변환 (모델이 순환성을 학습하는 데 도움을 줌)\n",
        "# df['month_sin'] = np.sin(2 * np.pi * df['month']/12)\n",
        "# df['month_cos'] = np.cos(2 * np.pi * df['month']/12)\n",
        "# df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week']/7)\n",
        "# df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week']/7)\n",
        "# print(\"날짜 특성 생성 완료.\")\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])].copy()\n",
        "test_df = df[df['year'] == 2023].copy()\n",
        "\n",
        "# --- 2. 특성(features) 목록 재정의 ---\n",
        "# 원본 날짜('call119_train.tm')는 제외하고 새로 만든 날짜 특성을 포함시킵니다.\n",
        "features = [col for col in df.columns if col not in [\n",
        "    'Unnamed: 0', 'call119_train.call_count', 'call119_train.address_city',\n",
        "    'call119_train.stn', 'call119_train.tm','day','month'# 원본 tm 컬럼 제외\n",
        "]]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "X_train = train_df[features].copy()\n",
        "Y_train = train_df[target].copy()\n",
        "\n",
        "X_test = test_df[features].copy()\n",
        "Y_test = test_df[target].copy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # object 타입을 category로 변환 (LightGBM/XGBoost가 범주형으로 인식)\n",
        "for col in X_train.select_dtypes(include='object').columns:\n",
        "    # 훈련셋과 테스트셋의 모든 범주를 파악하여 누락되는 값이 없도록 처리\n",
        "    all_categories = pd.concat([X_train[col], X_test[col]]).astype('category').cat.categories\n",
        "    X_train[col] = pd.Categorical(X_train[col], categories=all_categories)\n",
        "    X_test[col] = pd.Categorical(X_test[col], categories=all_categories)\n",
        "\n",
        "# # # --- LightGBM 모델 학습 및 평가 ---\n",
        "# # print(\"\\nLGBM 모델을 학습합니다...\")\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "# category 타입의 컬럼을 그대로 사용하여 학습\n",
        "lgb_model.fit(X_train, Y_train)\n",
        "Y_pred_lgb = lgb_model.predict(X_test)\n",
        "\n",
        "rmse_lgb = np.sqrt(mean_squared_error(Y_test, Y_pred_lgb))\n",
        "mae_lgb = mean_absolute_error(Y_test, Y_pred_lgb)\n",
        "\n",
        "\n",
        "print(f\"LGBMRegressor with Date Features RMSE: {rmse_lgb}\")\n",
        "print(f\"LGBMRegressor MAE: {mae_lgb}\")\n",
        "\n",
        "# --- XGBoost 모델 학습 및 평가 ---\n",
        "# XGBoost 1.5.0 이상 버전은 enable_categorical=True 옵션으로 category 타입을 직접 지원합니다.\n",
        "# print(\"\\nXGBoost 모델을 학습합니다...\")\n",
        "# xgb_model = xgb.XGBRegressor(random_state=42, enable_categorical=True, tree_method='hist')\n",
        "# xgb_model.fit(X_train, Y_train)\n",
        "# Y_pred_xgb = xgb_model.predict(X_test)\n",
        "# rmse_xgb = np.sqrt(mean_squared_error(Y_test, Y_pred_xgb))\n",
        "# print(f\"XGBRegressor with Date Features RMSE: {rmse_xgb}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOqwPC86W-xN",
        "outputId": "1adeaea6-1d77-40b1-b02f-49f9a387354c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "날짜 특성을 생성합니다...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001240 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1849\n",
            "[LightGBM] [Info] Number of data points in the train set: 32372, number of used features: 11\n",
            "[LightGBM] [Info] Start training from score 1.950358\n",
            "LGBMRegressor with Date Features RMSE: 1.1581444129694443\n",
            "LGBMRegressor MAE: 0.7946775614941148\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이거는 결측치를 재현님 알려주신 KNN inputter로 채워넣어서 했다. 일단 성능 자체는 결측치를 아무것도 안채워넣고 lightgbm이 알아서 처리하도록 하는것이 성능상 베스트로 나왔다. 각 모델마다 결측치를 처리하는 방법이 다르고, 어떻게 결측치를 처리하는냐에 따라 성능이 크게 좌우된다."
      ],
      "metadata": {
        "id": "GQMaYEwb4q5H"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwELJxn7rmxi",
        "outputId": "7e95f74c-b958-401c-c56f-10eb5da74c8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- LightGBM (LGBMRegressor) Hyperparameter Tuning ---\n",
            "Fitting 5 folds for each of 729 candidates, totalling 3645 fits\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001026 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1922\n",
            "[LightGBM] [Info] Number of data points in the train set: 32372, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 1.950358\n",
            "\n",
            "Best LGBM parameters found: {'learning_rate': 0.1, 'max_depth': -1, 'n_estimators': 200, 'num_leaves': 20, 'reg_alpha': 0, 'reg_lambda': 0}\n",
            "LGBMRegressor Tuned RMSE: 1.2551551383312003\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # 재사용\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/preprocessed_full_data_V3.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Extract year from 'call119_train.tm'\n",
        "df['year'] = df['call119_train.tm'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])].copy()\n",
        "test_df = df[df['year'] == 2023].copy()\n",
        "\n",
        "# Define features (X) and target (Y)\n",
        "features = [col for col in df.columns if col not in ['Unnamed: 0', 'year', 'call119_train.call_count']]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "X_train = train_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_train = train_df[target]\n",
        "\n",
        "X_test = test_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_test = test_df[target]\n",
        "\n",
        "# --- LightGBM (LGBMRegressor) 최적의 하이퍼파라미터 탐색 ---\n",
        "print(\"\\n--- LightGBM (LGBMRegressor) Hyperparameter Tuning ---\")\n",
        "\n",
        "# LGBMRegressor 모델 초기화\n",
        "lgb_model = lgb.LGBMRegressor(random_state=42)\n",
        "\n",
        "# 튜닝할 파라미터들의 조합을 정의합니다.\n",
        "# 이 파라미터 그리드는 시작점으로 사용되며, 필요에 따라 더 넓거나 좁게 설정할 수 있습니다.\n",
        "param_grid_lgb = {\n",
        "    'n_estimators': [100, 200, 300], # 부스팅 단계 수 (트리 개수)\n",
        "    'learning_rate': [0.01, 0.05, 0.1], # 각 부스팅 단계에서 학습률\n",
        "    'num_leaves': [20, 31, 40], # 한 트리의 최대 리프 노드 수 (LightGBM 특유의 파라미터, 깊이와 관련)\n",
        "    'max_depth': [-1, 10, 15], # 트리의 최대 깊이 (-1은 제한 없음)\n",
        "    'reg_alpha': [0, 0.1, 0.5], # L1 정규화 항\n",
        "    'reg_lambda': [0, 0.1, 0.5] # L2 정규화 항\n",
        "}\n",
        "\n",
        "# GridSearchCV 객체 생성\n",
        "# cv: 교차 검증 폴드 수 (예: 5)\n",
        "# scoring: 모델 성능 평가 지표 (회귀에서는 'neg_mean_squared_error'를 사용하면 MSE를 최소화)\n",
        "# n_jobs: 사용할 CPU 코어 수 (-1은 모든 코어 사용)\n",
        "# verbose: 학습 진행 상황 출력 레벨\n",
        "grid_search_lgb = GridSearchCV(estimator=lgb_model,\n",
        "                               param_grid=param_grid_lgb,\n",
        "                               cv=5, # 5-겹 교차 검증\n",
        "                               scoring='neg_mean_squared_error', # MSE를 최소화 (음수 값이므로 큰 값이 좋음)\n",
        "                               n_jobs=-1, # 모든 코어 사용\n",
        "                               verbose=2) # 진행 상황 출력\n",
        "\n",
        "# 그리드 서치 수행\n",
        "grid_search_lgb.fit(X_train, Y_train)\n",
        "\n",
        "# 최적의 파라미터 조합 출력\n",
        "print(f\"\\nBest LGBM parameters found: {grid_search_lgb.best_params_}\")\n",
        "\n",
        "# 최적의 모델로 예측 및 RMSE 계산\n",
        "best_lgbm_model = grid_search_lgb.best_estimator_\n",
        "Y_pred_lgbm_tuned = best_lgbm_model.predict(X_test)\n",
        "rmse_lgbm_tuned = np.sqrt(mean_squared_error(Y_test, Y_pred_lgbm_tuned))\n",
        "print(f\"LGBMRegressor Tuned RMSE: {rmse_lgbm_tuned}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "merged data로 각 카테고리별 분석후 성능을 더 뽑아낼려고 해도 성능이 오히려 더 떨어지는 역효과가 발생했습니다."
      ],
      "metadata": {
        "id": "gvz7mPUe4jim"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qHx6Qjs26hTg",
        "outputId": "913bc31e-1ff7-483e-983a-df1ab2e8e05e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-16-988815491>:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna('Unknown', inplace=True)\n",
            "<ipython-input-16-988815491>:27: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  df[col].fillna('Unknown', inplace=True)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ 전처리가 완료되었습니다. 결측치가 모두 채워졌습니다.\n",
            "   address_gu  sub_address  ta_max  ta_min  ta_max_min  hm_min  hm_max  \\\n",
            "0         0.0         34.0    23.7    16.6         7.1   63.20   90.30   \n",
            "1         0.0        121.0    24.3    15.4         8.9   64.20   96.40   \n",
            "2         1.0         16.0    25.6    17.2         8.4   46.60   76.40   \n",
            "3         1.0         17.0    25.3    17.1         8.2   49.42   85.54   \n",
            "4         1.0         21.0    26.5    16.7         9.8   52.14   93.72   \n",
            "\n",
            "   ws_max  ws_ins_max  rn_day  ...  cat_call_산악사고  cat_call_산불  cat_call_구급기타  \\\n",
            "0     5.8        10.6     0.0  ...            0.0          0.0            0.0   \n",
            "1     4.8         8.6     0.0  ...            0.0          0.0            0.0   \n",
            "2     4.8         8.3     0.0  ...            0.0          0.0            0.0   \n",
            "3     7.1         9.7     0.0  ...            0.0          0.0            0.0   \n",
            "4     5.7         8.1     0.0  ...            0.0          0.0            0.0   \n",
            "\n",
            "   cat_call_대민지원  cat_call_수난사고  cat_call_상황출동  cat_call_자연재해  cat_call_기타  \\\n",
            "0            0.0            0.0            0.0            0.0          0.0   \n",
            "1            0.0            0.0            0.0            0.0          0.0   \n",
            "2            0.0            0.0            0.0            0.0          0.0   \n",
            "3            0.0            0.0            0.0            0.0          0.0   \n",
            "4            0.0            0.0            0.0            0.0          0.0   \n",
            "\n",
            "   cat_call_지원출동(풍수해)  call_count  \n",
            "0                 0.0         1.0  \n",
            "1                 0.0         1.0  \n",
            "2                 0.0         1.0  \n",
            "3                 0.0         1.0  \n",
            "4                 0.0         3.0  \n",
            "\n",
            "[5 rows x 23 columns]\n",
            "\n",
            "--- 최종 예측 평가 ---\n",
            "📈 Root Mean Squared Error (RMSE): 1.8295\n",
            "📊 R-squared (R²): 0.0444\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/merged_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "\n",
        "# --- 전처리 파이프라인 시작 ---\n",
        "\n",
        "# 2. tm, address_city, stn 칼럼 제거\n",
        "df = df.drop(columns=['tm', 'address_city', 'stn'])\n",
        "\n",
        "# 3. -99 값을 np.nan으로 변환\n",
        "df.replace(-99, np.nan, inplace=True)\n",
        "\n",
        "# 4. address_gu, sub_address 라벨 인코딩\n",
        "# KNNImputer는 숫자형 데이터에서만 작동하므로, 문자열 결측치를 먼저 채워야 합니다.\n",
        "for col in ['address_gu', 'sub_address']:\n",
        "    # NaN 값을 'Unknown' 같은 문자열로 채웁니다.\n",
        "    df[col].fillna('Unknown', inplace=True)\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# 5. 날짜 특성 공학\n",
        "if 'date' in df.columns:\n",
        "    df['date'] = pd.to_datetime(df['date'])\n",
        "    df['year'] = df['date'].dt.year\n",
        "    df['month'] = df['date'].dt.month\n",
        "    df['day'] = df['date'].dt.day\n",
        "    df = df.drop(columns=['date'])\n",
        "\n",
        "# 6. KNNImputer로 결측치 채우기\n",
        "# 원본 데이터프레임의 컬럼 순서를 저장해둡니다.\n",
        "original_columns = df.columns\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "df_imputed = imputer.fit_transform(df)\n",
        "\n",
        "# 다시 데이터프레임으로 변환\n",
        "df = pd.DataFrame(df_imputed, columns=original_columns)\n",
        "\n",
        "print(\"✅ 전처리가 완료되었습니다. 결측치가 모두 채워졌습니다.\")\n",
        "print(df.head())\n",
        "\n",
        "# --- 모델링 및 예측 ---\n",
        "\n",
        "# 7. 피처와 타겟 정의\n",
        "cat_call_features = [\n",
        "  'cat_call_교통사고', 'cat_call_부상', 'cat_call_업무운행', 'cat_call_산악사고',\n",
        "  'cat_call_산불', 'cat_call_구급기타', 'cat_call_대민지원', 'cat_call_수난사고',\n",
        "  'cat_call_상황출동', 'cat_call_자연재해', 'cat_call_기타', 'cat_call_지원출동(풍수해)'\n",
        "]\n",
        "predictor_columns = [col for col in df.columns if col not in cat_call_features and col != 'call_count']\n",
        "\n",
        "X = df[predictor_columns]\n",
        "y_final = df['call_count']\n",
        "\n",
        "# 8. 데이터 분리\n",
        "X_train, X_test, y_train_final, y_test_final = train_test_split(X, y_final, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9. 모델 학습 및 개별 예측\n",
        "cat_call_predictions = {}\n",
        "for category in cat_call_features:\n",
        "    y_category = df[category]\n",
        "    y_category_train = y_category.loc[X_train.index]\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_category_train)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    cat_call_predictions[category] = predictions\n",
        "\n",
        "# 10. 최종 call_count 예측 (개별 예측의 합)\n",
        "predicted_call_count = np.sum(list(cat_call_predictions.values()), axis=0)\n",
        "\n",
        "# 11. 모델 평가 (RMSE, R-squared)\n",
        "mse = mean_squared_error(y_test_final, predicted_call_count)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_final, predicted_call_count)\n",
        "\n",
        "print(\"\\n--- 최종 예측 평가 ---\")\n",
        "print(f\"📈 Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"📊 R-squared (R²): {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "merged data로 각 카테고리별 분석후 성능을 더 뽑아낼려고 해도 성능이 오히려 더 떨어지는 역효과가 발생했습니다.\n"
      ],
      "metadata": {
        "id": "7_9s01EC4T7f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mwIbdZSf84QO",
        "outputId": "194bcf19-9f6d-4c92-ae79-01b974bcf011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After KNN Imputation and Inverse Scaling, DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 42924 entries, 0 to 42923\n",
            "Data columns (total 23 columns):\n",
            " #   Column              Non-Null Count  Dtype  \n",
            "---  ------              --------------  -----  \n",
            " 0   address_gu          42924 non-null  int64  \n",
            " 1   sub_address         42924 non-null  int64  \n",
            " 2   ta_max              42924 non-null  float64\n",
            " 3   ta_min              42924 non-null  float64\n",
            " 4   ta_max_min          42924 non-null  float64\n",
            " 5   hm_min              42924 non-null  float64\n",
            " 6   hm_max              42924 non-null  float64\n",
            " 7   ws_max              42924 non-null  float64\n",
            " 8   ws_ins_max          42924 non-null  float64\n",
            " 9   rn_day              42924 non-null  float64\n",
            " 10  cat_call_교통사고       42924 non-null  int64  \n",
            " 11  cat_call_부상         42924 non-null  int64  \n",
            " 12  cat_call_업무운행       42924 non-null  int64  \n",
            " 13  cat_call_산악사고       42924 non-null  int64  \n",
            " 14  cat_call_산불         42924 non-null  int64  \n",
            " 15  cat_call_구급기타       42924 non-null  int64  \n",
            " 16  cat_call_대민지원       42924 non-null  int64  \n",
            " 17  cat_call_수난사고       42924 non-null  int64  \n",
            " 18  cat_call_상황출동       42924 non-null  int64  \n",
            " 19  cat_call_자연재해       42924 non-null  int64  \n",
            " 20  cat_call_기타         42924 non-null  int64  \n",
            " 21  cat_call_지원출동(풍수해)  42924 non-null  int64  \n",
            " 22  call_count          42924 non-null  int64  \n",
            "dtypes: float64(8), int64(15)\n",
            "memory usage: 7.5 MB\n",
            "\n",
            "Missing values after KNN imputation per column:\n",
            "address_gu            0\n",
            "sub_address           0\n",
            "ta_max                0\n",
            "ta_min                0\n",
            "ta_max_min            0\n",
            "hm_min                0\n",
            "hm_max                0\n",
            "ws_max                0\n",
            "ws_ins_max            0\n",
            "rn_day                0\n",
            "cat_call_교통사고         0\n",
            "cat_call_부상           0\n",
            "cat_call_업무운행         0\n",
            "cat_call_산악사고         0\n",
            "cat_call_산불           0\n",
            "cat_call_구급기타         0\n",
            "cat_call_대민지원         0\n",
            "cat_call_수난사고         0\n",
            "cat_call_상황출동         0\n",
            "cat_call_자연재해         0\n",
            "cat_call_기타           0\n",
            "cat_call_지원출동(풍수해)    0\n",
            "call_count            0\n",
            "dtype: int64\n",
            "✅ 전처리가 완료되었습니다. 결측치가 모두 채워졌습니다.\n",
            "   address_gu  sub_address  ta_max  ta_min  ta_max_min  hm_min  hm_max  \\\n",
            "0           0           34    23.7    16.6         7.1   63.20   90.30   \n",
            "1           0          121    24.3    15.4         8.9   64.20   96.40   \n",
            "2           1           16    25.6    17.2         8.4   46.60   76.40   \n",
            "3           1           17    25.3    17.1         8.2   30.14   70.26   \n",
            "4           1           21    26.5    16.7         9.8   41.10   86.60   \n",
            "\n",
            "   ws_max  ws_ins_max  rn_day  ...  cat_call_산악사고  cat_call_산불  cat_call_구급기타  \\\n",
            "0     5.8        10.6     0.0  ...              0            0              0   \n",
            "1     4.8         8.6     0.0  ...              0            0              0   \n",
            "2     4.8         8.3     0.0  ...              0            0              0   \n",
            "3     7.1         9.7     0.0  ...              0            0              0   \n",
            "4     5.7         8.1     0.0  ...              0            0              0   \n",
            "\n",
            "   cat_call_대민지원  cat_call_수난사고  cat_call_상황출동  cat_call_자연재해  cat_call_기타  \\\n",
            "0              0              0              0              0            0   \n",
            "1              0              0              0              0            0   \n",
            "2              0              0              0              0            0   \n",
            "3              0              0              0              0            0   \n",
            "4              0              0              0              0            0   \n",
            "\n",
            "   cat_call_지원출동(풍수해)  call_count  \n",
            "0                   0           1  \n",
            "1                   0           1  \n",
            "2                   0           1  \n",
            "3                   0           1  \n",
            "4                   0           3  \n",
            "\n",
            "[5 rows x 23 columns]\n",
            "\n",
            "--- 최종 예측 평가 ---\n",
            "📈 Root Mean Squared Error (RMSE): 1.8311\n",
            "📊 R-squared (R²): 0.0428\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/merged_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "\n",
        "# --- 전처리 파이프라인 시작 ---\n",
        "\n",
        "# 2. tm, address_city, stn 칼럼 제거\n",
        "df = df.drop(columns=['tm', 'address_city', 'stn'])\n",
        "\n",
        "# 3. -99 값을 np.nan으로 변환\n",
        "df.replace(-99, np.nan, inplace=True)\n",
        "\n",
        "\n",
        "# Initialize LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder to 'call119_train.address_gu' and 'call119_train.sub_address'\n",
        "df['address_gu'] = label_encoder.fit_transform(df['address_gu'])\n",
        "df['sub_address'] = label_encoder.fit_transform(df['sub_address'])\n",
        "\n",
        "\n",
        "features_for_imputation = [\n",
        "    'ta_max',\n",
        "    'ta_min',\n",
        "    'ta_max_min',\n",
        "    'hm_min',\n",
        "    'hm_max',\n",
        "    'ws_max',\n",
        "    'ws_ins_max',\n",
        "    'rn_day'\n",
        "]\n",
        "\n",
        "# 결측치 처리 전 스케일링\n",
        "# StandardScaler 객체 생성\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# KNNImputer를 적용할 특성들만 추출하여 스케일링\n",
        "# 스케일링은 결측치가 없는 데이터에 먼저 fit되어야 하지만,\n",
        "# KNNImputer가 NaN 값을 처리할 수 있으므로, 전체 데이터에 바로 적용합니다.\n",
        "# 하지만, 일반적으로는 결측치를 먼저 처리한 후에 스케일링을 하는 것이 안전합니다.\n",
        "# KNNImputer는 NaN 값을 무시하고 거리 계산을 수행하므로,\n",
        "# 스케일링을 먼저 적용한 후 Impute 하는 것이 더 좋습니다 (NaN 값은 스케일링에 영향을 받지 않음).\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df[features_for_imputation]),\n",
        "                         columns=features_for_imputation,\n",
        "                         index=df.index)\n",
        "\n",
        "# KNNImputer 객체 생성 (n_neighbors는 이웃의 수, 기본값은 5)\n",
        "imputer = KNNImputer(n_neighbors=5, weights='uniform')\n",
        "\n",
        "# 스케일링된 데이터에 KNNImputer 적용\n",
        "df_imputed_scaled = pd.DataFrame(imputer.fit_transform(df_scaled),\n",
        "                                 columns=features_for_imputation,\n",
        "                                 index=df.index)\n",
        "\n",
        "# Imputation이 완료된 데이터를 다시 원래 스케일로 되돌립니다 (역변환)\n",
        "df_imputed_original_scale = pd.DataFrame(scaler.inverse_transform(df_imputed_scaled),\n",
        "                                         columns=features_for_imputation,\n",
        "                                         index=df.index)\n",
        "\n",
        "# 원본 DataFrame에 결측치가 채워지고 원래 스케일로 복원된 특성들을 업데이트합니다.\n",
        "for col in features_for_imputation:\n",
        "    df[col] = df_imputed_original_scale[col]\n",
        "\n",
        "# 결측치 처리 후 데이터 정보 확인\n",
        "print(\"After KNN Imputation and Inverse Scaling, DataFrame Info:\")\n",
        "df.info()\n",
        "\n",
        "# 확인을 위해 각 컬럼별 결측치 개수 출력\n",
        "print(\"\\nMissing values after KNN imputation per column:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "\n",
        "\n",
        "print(\"✅ 전처리가 완료되었습니다. 결측치가 모두 채워졌습니다.\")\n",
        "print(df.head())\n",
        "\n",
        "# --- 모델링 및 예측 ---\n",
        "\n",
        "# 7. 피처와 타겟 정의\n",
        "cat_call_features = [\n",
        "  'cat_call_교통사고', 'cat_call_부상', 'cat_call_업무운행', 'cat_call_산악사고',\n",
        "  'cat_call_산불', 'cat_call_구급기타', 'cat_call_대민지원', 'cat_call_수난사고',\n",
        "  'cat_call_상황출동', 'cat_call_자연재해', 'cat_call_기타', 'cat_call_지원출동(풍수해)'\n",
        "]\n",
        "predictor_columns = [col for col in df.columns if col not in cat_call_features and col != 'call_count']\n",
        "\n",
        "X = df[predictor_columns]\n",
        "y_final = df['call_count']\n",
        "\n",
        "# 8. 데이터 분리\n",
        "X_train, X_test, y_train_final, y_test_final = train_test_split(X, y_final, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9. 모델 학습 및 개별 예측\n",
        "cat_call_predictions = {}\n",
        "for category in cat_call_features:\n",
        "    y_category = df[category]\n",
        "    y_category_train = y_category.loc[X_train.index]\n",
        "\n",
        "    model = LinearRegression()\n",
        "    model.fit(X_train, y_category_train)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    cat_call_predictions[category] = predictions\n",
        "\n",
        "# 10. 최종 call_count 예측 (개별 예측의 합)\n",
        "predicted_call_count = np.sum(list(cat_call_predictions.values()), axis=0)\n",
        "\n",
        "# 11. 모델 평가 (RMSE, R-squared)\n",
        "mse = mean_squared_error(y_test_final, predicted_call_count)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_final, predicted_call_count)\n",
        "\n",
        "print(\"\\n--- 최종 예측 평가 ---\")\n",
        "print(f\"📈 Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"📊 R-squared (R²): {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "재홍님이 제공해주신 merge_data 로 돌려봤는데 카테고리별(산불,교통사고,자연재해 등등)로 나눠서 예측해보고 나중에 합해서 총 신고 건수를 예측해보니 오히려 정확도가 좀더 떨어지게 나왔다."
      ],
      "metadata": {
        "id": "6IBIpdTx2I7Z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "afvS--Io_Z8K",
        "outputId": "4505f261-8d2a-4462-d896-14df0e656785"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Preprocessing is complete. All missing values have been imputed.\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001180 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.523079\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004906 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.732433\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003764 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.227205\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008844 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.022482\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006850 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.001922\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.011254 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.166079\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006616 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.004397\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007911 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.035674\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.013705 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.036868\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003858 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.155246\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003708 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.004543\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.004509 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1913\n",
            "[LightGBM] [Info] Number of data points in the train set: 34339, number of used features: 10\n",
            "[LightGBM] [Info] Start training from score 0.011095\n",
            "\n",
            "--- Final Prediction Evaluation (LightGBM) ---\n",
            "📈 Root Mean Squared Error (RMSE): 1.5168\n",
            "📊 R-squared (R²): 0.3432\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import lightgbm as lgb\n",
        "\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/merged_data.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# --- 전처리 파이프라인 시작 ---\n",
        "\n",
        "# 2. 칼럼 제거\n",
        "df = df.drop(columns=['tm', 'address_city', 'stn'])\n",
        "\n",
        "# 3. -99 값을 np.nan으로 변환\n",
        "df.replace(-99, np.nan, inplace=True)\n",
        "\n",
        "# 4. 라벨 인코딩 (결측치 처리 포함)\n",
        "for col in ['address_gu', 'sub_address']:\n",
        "    # LabelEncoder는 NaN 값을 처리할 수 없으므로, 먼저 문자열로 채웁니다.\n",
        "    df[col] = df[col].astype(str).fillna('Unknown')\n",
        "    le = LabelEncoder()\n",
        "    df[col] = le.fit_transform(df[col])\n",
        "\n",
        "# 5. KNN Imputation을 위한 스케일링 및 결측치 처리\n",
        "features_for_imputation = [\n",
        "    'ta_max', 'ta_min', 'ta_max_min', 'hm_min',\n",
        "    'hm_max', 'ws_max', 'ws_ins_max', 'rn_day'\n",
        "]\n",
        "\n",
        "# StandardScaler와 KNNImputer 객체 생성\n",
        "scaler = StandardScaler()\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "\n",
        "# 스케일링 -> 결측치 채우기 -> 역스케일링\n",
        "df_to_impute = df[features_for_imputation]\n",
        "df_scaled = scaler.fit_transform(df_to_impute)\n",
        "df_imputed_scaled = imputer.fit_transform(df_scaled)\n",
        "df_imputed_original_scale = scaler.inverse_transform(df_imputed_scaled)\n",
        "\n",
        "# 원본 데이터프레임에 결측치가 채워진 값으로 업데이트\n",
        "df[features_for_imputation] = df_imputed_original_scale\n",
        "\n",
        "# # 6. 날짜 특성 생성\n",
        "# if 'date' in df.columns:\n",
        "#     df['date'] = pd.to_datetime(df['date'])\n",
        "#     df['year'] = df['date'].dt.year\n",
        "#     df['month'] = df['date'].dt.month\n",
        "#     df['day'] = df['date'].dt.day\n",
        "#     df = df.drop(columns=['date'])\n",
        "\n",
        "\n",
        "print(\"✅ Preprocessing is complete. All missing values have been imputed.\")\n",
        "\n",
        "# --- 모델링 및 예측 ---\n",
        "\n",
        "# 7. 피처와 타겟 정의\n",
        "cat_call_features = [\n",
        "  'cat_call_교통사고', 'cat_call_부상', 'cat_call_업무운행', 'cat_call_산악사고',\n",
        "  'cat_call_산불', 'cat_call_구급기타', 'cat_call_대민지원', 'cat_call_수난사고',\n",
        "  'cat_call_상황출동', 'cat_call_자연재해', 'cat_call_기타', 'cat_call_지원출동(풍수해)'\n",
        "]\n",
        "predictor_columns = [col for col in df.columns if col not in cat_call_features and col != 'call_count']\n",
        "\n",
        "X = df[predictor_columns]\n",
        "y_final = df['call_count']\n",
        "\n",
        "# 8. 데이터 분리\n",
        "X_train, X_test, y_train_final, y_test_final = train_test_split(X, y_final, test_size=0.2, random_state=42)\n",
        "\n",
        "# 9. LightGBM 모델 학습 및 개별 예측\n",
        "cat_call_predictions = {}\n",
        "for category in cat_call_features:\n",
        "    y_category = df[category]\n",
        "    y_category_train = y_category.loc[X_train.index]\n",
        "\n",
        "    # 모델을 LinearRegression에서 LGBMRegressor로 변경\n",
        "    model = lgb.LGBMRegressor(random_state=42)\n",
        "    model.fit(X_train, y_category_train)\n",
        "\n",
        "    predictions = model.predict(X_test)\n",
        "    cat_call_predictions[category] = predictions\n",
        "\n",
        "# 10. 최종 call_count 예측 (개별 예측의 합)\n",
        "predicted_call_count = np.sum(list(cat_call_predictions.values()), axis=0)\n",
        "\n",
        "# 11. 모델 평가 (RMSE, R-squared)\n",
        "mse = mean_squared_error(y_test_final, predicted_call_count)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test_final, predicted_call_count)\n",
        "\n",
        "print(\"\\n--- Final Prediction Evaluation (LightGBM) ---\")\n",
        "print(f\"📈 Root Mean Squared Error (RMSE): {rmse:.4f}\")\n",
        "print(f\"📊 R-squared (R²): {r2:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "여기부분은 그냥 넘어가라."
      ],
      "metadata": {
        "id": "jL3KEfvT18YB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "O6-dlT9OJHyG",
        "outputId": "d3de3b1b-fe06-486e-8096-946ecb35363e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_data' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-507ed912df14>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Train the predictor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;31m# AutoGluon will automatically perform feature engineering, model selection, and hyperparameter tuning.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpresets\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'best_quality'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;31m# Make predictions on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import lightgbm as lgb\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV # 재사용\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/call119_train1.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.replace(-99, np.nan, inplace=True)\n",
        "\n",
        "df = df.drop(columns=[ 'call119_train.address_city', 'call119_train.stn'])\n",
        "\n",
        "\n",
        "# Extract year from 'call119_train.tm'\n",
        "df['year'] = df['call119_train.tm'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])].copy()\n",
        "test_df = df[df['year'] == 2023].copy()\n",
        "\n",
        "# Define features (X) and target (Y)\n",
        "features = [col for col in df.columns if col not in ['Unnamed: 0', 'year', 'call119_train.call_count']]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "X_train = train_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_train = train_df[target]\n",
        "\n",
        "X_test = test_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_test = test_df[target]\n",
        "\n",
        "\n",
        "predictor = TabularPredictor(label=target, eval_metric='root_mean_squared_error', path='AutogluonModels/call_count_prediction')\n",
        "\n",
        "# Train the predictor\n",
        "# AutoGluon will automatically perform feature engineering, model selection, and hyperparameter tuning.\n",
        "predictor.fit(train_data=train_data, presets='best_quality')\n",
        "\n",
        "# Make predictions on the test set\n",
        "# For evaluation, we pass the test_data directly. AutoGluon will extract features and make predictions.\n",
        "predictions = predictor.predict(test_data_features_only)\n",
        "\n",
        "# Evaluate the model on the test set\n",
        "# AutoGluon can also evaluate the performance on a provided dataset\n",
        "leaderboard = predictor.leaderboard(test_data, silent=True)\n",
        "print(leaderboard)\n",
        "\n",
        "# You can also get the overall test score directly\n",
        "test_performance = predictor.evaluate(test_data, silent=True)\n",
        "print(f\"Test performance (RMSE): {test_performance['root_mean_squared_error']}\")\n",
        "\n",
        "print(\"\\nAutoGluon has completed training and evaluation.\")\n",
        "print(\"The 'predictions' variable contains the predicted call counts for the 2023 data.\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이거 깔아야 마지막 셀이 돌아간다."
      ],
      "metadata": {
        "id": "GO9gcjUw1uRa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bea71f88",
        "outputId": "b40ae53d-9adb-4b80-8389-4a5f7e464f16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting autogluon\n",
            "  Downloading autogluon-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.core==1.3.1 (from autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading autogluon.core-1.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting autogluon.features==1.3.1 (from autogluon)\n",
            "  Downloading autogluon.features-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting autogluon.tabular==1.3.1 (from autogluon.tabular[all]==1.3.1->autogluon)\n",
            "  Downloading autogluon.tabular-1.3.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting autogluon.multimodal==1.3.1 (from autogluon)\n",
            "  Downloading autogluon.multimodal-1.3.1-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting autogluon.timeseries==1.3.1 (from autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading autogluon.timeseries-1.3.1-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<2.3.0,>=1.25.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.16,>=1.5.4 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<1.7.0,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (1.6.1)\n",
            "Requirement already satisfied: networkx<4,>=3.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (3.4.2)\n",
            "Requirement already satisfied: pandas<2.3.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (2.2.2)\n",
            "Requirement already satisfied: tqdm<5,>=4.38 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (2.32.3)\n",
            "Requirement already satisfied: matplotlib<3.11,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (3.10.0)\n",
            "Collecting boto3<2,>=1.10 (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading boto3-1.38.36-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting autogluon.common==1.3.1 (from autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading autogluon.common-1.3.1-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting ray<2.45,>=2.10.0 (from ray[default]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading ray-2.44.1-cp311-cp311-manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: hyperopt<0.2.8,>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from autogluon.core[all]==1.3.1->autogluon) (0.2.7)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.core[all]==1.3.1->autogluon) (18.1.0)\n",
            "Requirement already satisfied: Pillow<12,>=10.0.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (11.1.0)\n",
            "Requirement already satisfied: torch<2.7,>=2.2 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (2.5.1+cu124)\n",
            "Collecting lightning<2.7,>=2.2 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: transformers<4.50,>=4.38.0 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.3.1->autogluon) (4.48.3)\n",
            "Requirement already satisfied: accelerate<2.0,>=0.34.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: jsonschema<4.24,>=4.18 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (4.23.0)\n",
            "Collecting seqeval<1.3.0,>=1.2.2 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading seqeval-1.2.2.tar.gz (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting evaluate<0.5.0,>=0.4.0 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Collecting timm<1.0.7,>=0.9.5 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading timm-1.0.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchvision<0.22.0,>=0.16.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (0.20.1+cu124)\n",
            "Requirement already satisfied: scikit-image<0.26.0,>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (0.25.2)\n",
            "Requirement already satisfied: text-unidecode<1.4,>=1.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (1.3)\n",
            "Collecting torchmetrics<1.8,>=1.2.0 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting omegaconf<2.4.0,>=2.1.1 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting pytorch-metric-learning<2.9,>=1.3.0 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting nlpaug<1.2.0,>=1.1.10 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nlpaug-1.1.11-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting nltk<3.9,>=3.4.5 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting openmim<0.4.0,>=0.3.7 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading openmim-0.3.9-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: defusedxml<0.7.2,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (0.7.1)\n",
            "Requirement already satisfied: jinja2<3.2,>=3.0.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (3.1.5)\n",
            "Requirement already satisfied: tensorboard<3,>=2.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.multimodal==1.3.1->autogluon) (2.18.0)\n",
            "Collecting pytesseract<0.4,>=0.3.9 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting nvidia-ml-py3<8.0,>=7.352.0 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia-ml-py3-7.352.0.tar.gz (19 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pdf2image<1.19,>=1.17.0 (from autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting catboost<1.3,>=1.2 (from autogluon.tabular[all]==1.3.1->autogluon)\n",
            "  Downloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: einops<0.9,>=0.7 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.3.1->autogluon) (0.8.1)\n",
            "Requirement already satisfied: xgboost<3.1,>=2.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.3.1->autogluon) (2.1.4)\n",
            "Requirement already satisfied: fastai<2.9,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.3.1->autogluon) (2.7.18)\n",
            "Requirement already satisfied: huggingface-hub[torch] in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.3.1->autogluon) (0.28.1)\n",
            "Requirement already satisfied: lightgbm<4.7,>=4.0 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.3.1->autogluon) (4.5.0)\n",
            "Requirement already satisfied: spacy<3.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.tabular[all]==1.3.1->autogluon) (3.7.5)\n",
            "Requirement already satisfied: joblib<2,>=1.1 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (1.4.2)\n",
            "Collecting pytorch-lightning (from autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting gluonts<0.17,>=0.15.0 (from autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading gluonts-0.16.1-py3-none-any.whl.metadata (9.8 kB)\n",
            "Collecting statsforecast<2.0.2,>=1.7.0 (from autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading statsforecast-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (29 kB)\n",
            "Collecting mlforecast<0.14,>0.13 (from autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading mlforecast-0.13.6-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting utilsforecast<0.2.11,>=0.2.3 (from autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading utilsforecast-0.2.10-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting coreforecast<0.0.16,>=0.0.12 (from autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading coreforecast-0.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Collecting fugue>=0.9.0 (from autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading fugue-0.9.1-py3-none-any.whl.metadata (18 kB)\n",
            "Requirement already satisfied: orjson~=3.9 in /usr/local/lib/python3.11/dist-packages (from autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (3.10.15)\n",
            "Requirement already satisfied: psutil<7.1.0,>=5.7.3 in /usr/local/lib/python3.11/dist-packages (from autogluon.common==1.3.1->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (5.9.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.3.1->autogluon) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.3.1->autogluon) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate<2.0,>=0.34.0->autogluon.multimodal==1.3.1->autogluon) (0.5.3)\n",
            "Collecting botocore<1.39.0,>=1.38.36 (from boto3<2,>=1.10->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading botocore-1.38.36-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<2,>=1.10->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.14.0,>=0.13.0 (from boto3<2,>=1.10->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading s3transfer-0.13.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.3.1->autogluon) (0.20.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.3.1->autogluon) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost<1.3,>=1.2->autogluon.tabular[all]==1.3.1->autogluon) (1.17.0)\n",
            "Collecting datasets>=2.0.0 (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting dill (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting xxhash (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading multiprocess-0.70.18-py311-none-any.whl.metadata (7.5 kB)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.3.1->autogluon) (2024.10.0)\n",
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.3.1->autogluon) (24.1.2)\n",
            "Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.3.1->autogluon) (0.0.7)\n",
            "Requirement already satisfied: fastcore<1.8,>=1.5.29 in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.3.1->autogluon) (1.7.29)\n",
            "Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.11/dist-packages (from fastai<2.9,>=2.3.1->autogluon.tabular[all]==1.3.1->autogluon) (1.0.3)\n",
            "Collecting triad>=0.9.7 (from fugue>=0.9.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading triad-0.9.8-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting adagio>=0.2.4 (from fugue>=0.9.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading adagio-0.2.6-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pydantic<3,>=1.7 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (2.10.6)\n",
            "Requirement already satisfied: toolz~=0.10 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gluonts<0.17,>=0.15.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (4.12.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.3.1->autogluon) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.3.1->autogluon) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.11/dist-packages (from hyperopt<0.2.8,>=0.2.7->autogluon.core[all]==1.3.1->autogluon) (0.10.9.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2<3.2,>=3.0.3->autogluon.multimodal==1.3.1->autogluon) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.3.1->autogluon) (25.1.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.3.1->autogluon) (2024.10.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.3.1->autogluon) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema<4.24,>=4.18->autogluon.multimodal==1.3.1->autogluon) (0.23.1)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib<3.11,>=3.7.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (2.8.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from mlforecast<0.14,>0.13->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (0.60.0)\n",
            "Collecting optuna (from mlforecast<0.14,>0.13->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting window-ops (from mlforecast<0.14,>0.13->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading window_ops-0.0.15-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: gdown>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.3.1->autogluon) (5.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk<3.9,>=3.4.5->autogluon.multimodal==1.3.1->autogluon) (8.1.8)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk<3.9,>=3.4.5->autogluon.multimodal==1.3.1->autogluon) (2024.11.6)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from omegaconf<2.4.0,>=2.1.1->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting colorama (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting model-index (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading model_index-0.1.11-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting opendatalab (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading opendatalab-0.0.10-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon) (13.9.4)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon) (0.9.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<2.3.0,>=2.0.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (3.17.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (1.1.0)\n",
            "Requirement already satisfied: protobuf!=3.19.5,>=3.15.3 in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (4.25.6)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (1.3.2)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.11/dist-packages (from ray<2.45,>=2.10.0->ray[default]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (1.5.0)\n",
            "Requirement already satisfied: aiohttp>=3.7 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (3.11.13)\n",
            "Collecting aiohttp_cors (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading aiohttp_cors-0.8.1-py3-none-any.whl.metadata (20 kB)\n",
            "Collecting colorful (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading colorful-0.5.6-py2.py3-none-any.whl.metadata (16 kB)\n",
            "Collecting py-spy>=0.2.0 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl.metadata (16 kB)\n",
            "Requirement already satisfied: grpcio>=1.42.0 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (1.70.0)\n",
            "Collecting opencensus (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading opencensus-0.11.4-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: prometheus_client>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (0.21.1)\n",
            "Requirement already satisfied: smart_open in /usr/local/lib/python3.11/dist-packages (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (7.1.0)\n",
            "Collecting virtualenv!=20.21.1,>=20.0.24 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading virtualenv-20.31.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting tensorboardX>=1.9 (from ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (2025.1.31)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.3.1->autogluon) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.3.1->autogluon) (2025.2.18)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image<0.26.0,>=0.19.1->autogluon.multimodal==1.3.1->autogluon) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<1.7.0,>=1.4.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (1.0.12)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (0.15.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (75.1.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (3.5.0)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (0.14.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.3.1->autogluon) (1.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.3.1->autogluon) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.3.1->autogluon) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<3,>=2.9->autogluon.multimodal==1.3.1->autogluon) (3.1.3)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<2.7,>=2.2->autogluon.multimodal==1.3.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<4.50,>=4.38.0->transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.3.1->autogluon) (0.21.0)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]<4.50,>=4.38.0->autogluon.multimodal==1.3.1->autogluon) (0.2.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (2.4.6)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp>=3.7->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (1.18.3)\n",
            "Collecting dill (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting multiprocess (from evaluate<0.5.0,>=0.4.0->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.3.1->autogluon) (4.13.3)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.11/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (1.3.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->mlforecast<0.14,>0.13->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (0.43.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.7->gluonts<0.17,>=0.15.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (2.27.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.11/dist-packages (from statsmodels>=0.13.2->statsforecast<2.0.2,>=1.7.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (1.0.1)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (0.1.5)\n",
            "Collecting fs (from triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading fs-2.4.16-py2.py3-none-any.whl.metadata (6.3 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0.0,>=0.3.0->spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (1.5.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon) (2.18.0)\n",
            "Collecting distlib<1,>=0.3.7 (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading distlib-0.3.9-py2.py3-none-any.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: platformdirs<5,>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from virtualenv!=20.21.1,>=20.0.24->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (4.3.6)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from weasel<0.5.0,>=0.1.0->spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (0.20.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart_open->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (1.17.2)\n",
            "Collecting ordered-set (from model-index->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading ordered_set-4.1.0-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting opencensus-context>=0.1.3 (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading opencensus_context-0.1.3-py2.py3-none-any.whl.metadata (3.3 kB)\n",
            "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (2.24.1)\n",
            "Collecting pycryptodome (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading openxlab-0.1.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna->mlforecast<0.14,>0.13->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading alembic-1.16.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna->mlforecast<0.14,>0.13->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna->mlforecast<0.14,>0.13->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (2.0.38)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost<1.3,>=1.2->autogluon.tabular[all]==1.3.1->autogluon) (9.0.0)\n",
            "Collecting Mako (from alembic>=1.5.0->optuna->mlforecast<0.14,>0.13->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading mako-1.3.10-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (1.69.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (1.26.0)\n",
            "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /usr/local/lib/python3.11/dist-packages (from google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (2.38.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.9->autogluon.tabular[all]==1.3.1->autogluon) (1.2.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon) (0.1.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna->mlforecast<0.14,>0.13->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon) (3.1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.3.1->autogluon) (2.6)\n",
            "Collecting appdirs~=1.4.3 (from fs->triad>=0.9.7->fugue>=0.9.0->autogluon.timeseries==1.3.1->autogluon.timeseries[all]==1.3.1->autogluon)\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting filelock (from ray<2.45,>=2.10.0->ray[default]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading filelock-3.14.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting oss2~=2.17.0 (from openxlab->opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading oss2-2.17.0.tar.gz (259 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytz>=2020.1 (from pandas<2.3.0,>=2.0.0->autogluon.core==1.3.1->autogluon.core[all]==1.3.1->autogluon)\n",
            "  Downloading pytz-2023.4-py2.py3-none-any.whl.metadata (22 kB)\n",
            "INFO: pip is looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting openxlab (from opendatalab->openmim<0.4.0,>=0.3.7->autogluon.multimodal==1.3.1->autogluon)\n",
            "  Downloading openxlab-0.1.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.38-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.37-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.36-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.35-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.34-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: pip is still looking at multiple versions of openxlab to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading openxlab-0.0.33-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.32-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.31-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.30-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.29-py3-none-any.whl.metadata (3.8 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading openxlab-0.0.28-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.27-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.26-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.25-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.24-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.23-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.22-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.21-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.20-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.19-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.18-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.17-py3-none-any.whl.metadata (3.7 kB)\n",
            "  Downloading openxlab-0.0.16-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.15-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.14-py3-none-any.whl.metadata (3.8 kB)\n",
            "  Downloading openxlab-0.0.13-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading openxlab-0.0.12-py3-none-any.whl.metadata (4.5 kB)\n",
            "  Downloading openxlab-0.0.11-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=4.0.0->nlpaug<1.2.0,>=1.1.10->autogluon.multimodal==1.3.1->autogluon) (1.7.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (0.4.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (4.9)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus->ray[default,tune]<2.45,>=2.10.0; extra == \"all\"->autogluon.core[all]==1.3.1->autogluon) (0.6.1)\n",
            "Downloading autogluon-1.3.1-py3-none-any.whl (9.8 kB)\n",
            "Downloading autogluon.core-1.3.1-py3-none-any.whl (222 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m222.7/222.7 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.features-1.3.1-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.multimodal-1.3.1-py3-none-any.whl (454 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m454.5/454.5 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.tabular-1.3.1-py3-none-any.whl (382 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m382.4/382.4 kB\u001b[0m \u001b[31m26.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.timeseries-1.3.1-py3-none-any.whl (181 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading autogluon.common-1.3.1-py3-none-any.whl (69 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.1/69.1 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading boto3-1.38.36-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.9/139.9 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading catboost-1.2.8-cp311-cp311-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coreforecast-0.0.15-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (275 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m275.8/275.8 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fugue-0.9.1-py3-none-any.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.2/278.2 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gluonts-0.16.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mlforecast-0.13.6-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.4/71.4 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nlpaug-1.1.11-py3-none-any.whl (410 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.5/410.5 kB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openmim-0.3.9-py2.py3-none-any.whl (52 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.7/52.7 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading pytorch_metric_learning-2.8.1-py3-none-any.whl (125 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ray-2.44.1-cp311-cp311-manylinux2014_x86_64.whl (68.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.1/68.1 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading statsforecast-2.0.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (354 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m354.4/354.4 kB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading timm-1.0.3-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m64.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m60.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading utilsforecast-0.2.10-py3-none-any.whl (41 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading adagio-0.2.6-py3-none-any.whl (19 kB)\n",
            "Downloading botocore-1.38.36-py3-none-any.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading datasets-3.6.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading py_spy-0.4.0-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (2.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.7/2.7 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading s3transfer-0.13.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading triad-0.9.8-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading virtualenv-20.31.2-py3-none-any.whl (6.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_cors-0.8.1-py3-none-any.whl (25 kB)\n",
            "Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Downloading colorful-0.5.6-py2.py3-none-any.whl (201 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.4/201.4 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading model_index-0.1.11-py3-none-any.whl (34 kB)\n",
            "Downloading opencensus-0.11.4-py2.py3-none-any.whl (128 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opendatalab-0.0.10-py3-none-any.whl (29 kB)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading window_ops-0.0.15-py3-none-any.whl (15 kB)\n",
            "Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.1-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading distlib-0.3.9-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 kB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencensus_context-0.1.3-py2.py3-none-any.whl (5.1 kB)\n",
            "Downloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Downloading fs-2.4.16-py2.py3-none-any.whl (135 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m135.3/135.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openxlab-0.0.11-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ordered_set-4.1.0-py3-none-any.whl (7.6 kB)\n",
            "Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m89.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Downloading mako-1.3.10-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.5/78.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: nvidia-ml-py3, antlr4-python3-runtime, seqeval\n",
            "  Building wheel for nvidia-ml-py3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nvidia-ml-py3: filename=nvidia_ml_py3-7.352.0-py3-none-any.whl size=19173 sha256=b54b7c585e66864ef5ee4f4a5d9ee5797c612106b5a70079790033345511a511\n",
            "  Stored in directory: /root/.cache/pip/wheels/47/50/9e/29dc79037d74c3c1bb4a8661fb608e8674b7e4260d6a3f8f51\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=26c7d383bca62c71ab978400ddfe85bbcabd9335326baf54d44192c2b9026dd7\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/97/32/461f837398029ad76911109f07047fde1d7b661a147c7c56d1\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=26ca73f2c93e421a95c8ed0ec43c44ee7f4eb9b3360bbb5e3bcd7357f243c206\n",
            "  Stored in directory: /root/.cache/pip/wheels/bc/92/f0/243288f899c2eacdfa8c5f9aede4c71a9bad0ee26a01dc5ead\n",
            "Successfully built nvidia-ml-py3 antlr4-python3-runtime seqeval\n",
            "Installing collected packages: py-spy, opencensus-context, nvidia-ml-py3, distlib, colorful, appdirs, antlr4-python3-runtime, xxhash, virtualenv, tensorboardX, pytesseract, pycryptodome, pdf2image, ordered-set, openxlab, omegaconf, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nltk, Mako, lightning-utilities, jmespath, fs, dill, coreforecast, colorlog, colorama, window-ops, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, model-index, botocore, alembic, utilsforecast, triad, seqeval, s3transfer, optuna, opendatalab, nvidia-cusolver-cu12, gluonts, catboost, aiohttp_cors, ray, openmim, opencensus, nlpaug, mlforecast, datasets, boto3, adagio, torchmetrics, pytorch-metric-learning, fugue, evaluate, autogluon.common, timm, statsforecast, pytorch-lightning, autogluon.features, autogluon.core, lightning, autogluon.tabular, autogluon.multimodal, autogluon.timeseries, autogluon\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.9.1\n",
            "    Uninstalling nltk-3.9.1:\n",
            "      Successfully uninstalled nltk-3.9.1\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: timm\n",
            "    Found existing installation: timm 1.0.15\n",
            "    Uninstalling timm-1.0.15:\n",
            "      Successfully uninstalled timm-1.0.15\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "textblob 0.19.0 requires nltk>=3.9, but you have nltk 3.8.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed Mako-1.3.10 adagio-0.2.6 aiohttp_cors-0.8.1 alembic-1.16.1 antlr4-python3-runtime-4.9.3 appdirs-1.4.4 autogluon-1.3.1 autogluon.common-1.3.1 autogluon.core-1.3.1 autogluon.features-1.3.1 autogluon.multimodal-1.3.1 autogluon.tabular-1.3.1 autogluon.timeseries-1.3.1 boto3-1.38.36 botocore-1.38.36 catboost-1.2.8 colorama-0.4.6 colorful-0.5.6 colorlog-6.9.0 coreforecast-0.0.15 datasets-3.6.0 dill-0.3.8 distlib-0.3.9 evaluate-0.4.3 fs-2.4.16 fugue-0.9.1 gluonts-0.16.1 jmespath-1.0.1 lightning-2.5.1.post0 lightning-utilities-0.14.3 mlforecast-0.13.6 model-index-0.1.11 multiprocess-0.70.16 nlpaug-1.1.11 nltk-3.8.1 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-ml-py3-7.352.0 nvidia-nvjitlink-cu12-12.4.127 omegaconf-2.3.0 opencensus-0.11.4 opencensus-context-0.1.3 opendatalab-0.0.10 openmim-0.3.9 openxlab-0.0.11 optuna-4.4.0 ordered-set-4.1.0 pdf2image-1.17.0 py-spy-0.4.0 pycryptodome-3.23.0 pytesseract-0.3.13 pytorch-lightning-2.5.1.post0 pytorch-metric-learning-2.8.1 ray-2.44.1 s3transfer-0.13.0 seqeval-1.2.2 statsforecast-2.0.1 tensorboardX-2.6.4 timm-1.0.3 torchmetrics-1.7.3 triad-0.9.8 utilsforecast-0.2.10 virtualenv-20.31.2 window-ops-0.0.15 xxhash-3.5.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins",
                  "pynvml"
                ]
              },
              "id": "1bfbb0853bec4a55a81b8a5e01206cc2"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install autogluon"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "AutoGluon 이라고 거의 모든 머신러닝 모델이랑 딥러닝 모델중에서 제일 성능이 괜찮은 모델로 자동 조합해주는 라이브러리이다. Firecast 팀즈단체방에 올려준 call119_train1.csv 만으로 돌려봤다. AutoGluon은 결측치를 알아서 처리해주기 때문에 결측치인 -99를 np.nan 으로 바꿔놓기만 하면 된다. 그리고 문자열 칼럼들을 정수로 인코딩해줘야 되는 작업들을 굳이 안해도 autogluon이 알아서 해준다는 장점이 있다."
      ],
      "metadata": {
        "id": "La1ond7Nz4PE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa4m2Vo_QDxQ",
        "outputId": "c7317600-5ed8-415f-82d2-0ef999ebd028"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Verbosity: 2 (Standard Logging)\n",
            "=================== System Info ===================\n",
            "AutoGluon Version:  1.3.1\n",
            "Python Version:     3.11.11\n",
            "Operating System:   Linux\n",
            "Platform Machine:   x86_64\n",
            "Platform Version:   #1 SMP PREEMPT_DYNAMIC Sun Mar 30 16:01:29 UTC 2025\n",
            "CPU Count:          2\n",
            "Memory Avail:       10.83 GB / 12.67 GB (85.5%)\n",
            "Disk Space Avail:   68.03 GB / 107.72 GB (63.2%)\n",
            "===================================================\n",
            "Presets specified: ['best_quality']\n",
            "Setting dynamic_stacking from 'auto' to True. Reason: Enable dynamic_stacking when use_bag_holdout is disabled. (use_bag_holdout=False)\n",
            "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
            "DyStack is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
            "\tThis is used to identify the optimal `num_stack_levels` value. Copies of AutoGluon will be fit on subsets of the data. Then holdout validation data is used to detect stacked overfitting.\n",
            "\tRunning DyStack for up to 900s of the 3600s of remaining time (25%).\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting AutoGluon training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\tRunning DyStack sub-fit in a ray process to avoid memory leakage. Enabling ray logging (enable_ray_logging=True). Specify `ds_args={'enable_ray_logging': False}` if you experience logging issues.\n",
            "2025-06-16 15:54:17,935\tINFO worker.py:1843 -- Started a local Ray instance. View the dashboard at \u001b[1m\u001b[32mhttp://127.0.0.1:8265 \u001b[39m\u001b[22m\n",
            "\t\tContext path: \"/content/AutogluonModels/firecast_prediction/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Running DyStack sub-fit ...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Beginning AutoGluon training ... Time limit = 894s\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m AutoGluon will save models to \"/content/AutogluonModels/firecast_prediction/ds_sub_fit/sub_fit_ho\"\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Train Data Rows:    28775\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Train Data Columns: 11\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Label Column:       call119_train.call_count\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Problem Type:       regression\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Preprocessing data ...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Using Feature Generators to preprocess the data ...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting AutoMLPipelineFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tAvailable Memory:                    10591.24 MB\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tTrain Data (Original)  Memory Usage: 6.80 MB (0.1% of available memory)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tStage 1 Generators:\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\tFitting AsTypeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tStage 2 Generators:\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\tFitting FillNaFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tStage 3 Generators:\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\tFitting IdentityFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\tFitting CategoryFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tStage 4 Generators:\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\tFitting DropUniqueFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tStage 5 Generators:\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tTypes of features in original data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\t('float', [])  : 8 | ['call119_train.ta_max', 'call119_train.ta_min', 'call119_train.ta_max_min', 'call119_train.hm_min', 'call119_train.hm_max', ...]\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\t('int', [])    : 1 | ['year']\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\t('object', []) : 2 | ['call119_train.address_gu', 'call119_train.sub_address']\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\t('category', []) : 2 | ['call119_train.address_gu', 'call119_train.sub_address']\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\t('float', [])    : 8 | ['call119_train.ta_max', 'call119_train.ta_min', 'call119_train.ta_max_min', 'call119_train.hm_min', 'call119_train.hm_max', ...]\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t\t('int', [])      : 1 | ['year']\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.2s = Fit runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t11 features in original data used to generate 11 features in processed data.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tTrain Data (Processed) Memory Usage: 2.06 MB (0.0% of available memory)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Data preprocessing and feature engineering runtime = 0.23s ...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tTo change this, specify the eval_metric parameter of Predictor()\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m User-specified model hyperparameters to be fit:\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m {\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m }\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 595.68s of the 893.72s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-2.0043\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.07s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.63s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 590.07s of the 888.11s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-2.0248\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.06s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.59s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 589.38s of the 887.42s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.19%)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=23597)\u001b[0m [1000]\tvalid_set's rmse: 1.7651\n",
            "\u001b[36m(_ray_fit pid=24052)\u001b[0m [1000]\tvalid_set's rmse: 2.20147\n",
            "\u001b[36m(_ray_fit pid=24052)\u001b[0m [2000]\tvalid_set's rmse: 2.17335\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.6508\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t57.78s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t3.48s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: LightGBM_BAG_L1 ... Training model for up to 526.85s of the 824.89s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.19%)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=24229)\u001b[0m [1000]\tvalid_set's rmse: 1.66156\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.64\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t62.86s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t1.73s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 460.49s of the 758.54s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m /usr/local/lib/python3.11/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.8746\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t117.76s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t1.65s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: CatBoost_BAG_L1 ... Training model for up to 336.22s of the 634.27s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=4.11%)\n",
            "\u001b[36m(_ray_fit pid=25748)\u001b[0m \tRan out of time, early stopping on iteration 1705.\n",
            "\u001b[36m(_ray_fit pid=25932)\u001b[0m \tRan out of time, early stopping on iteration 1675.\n",
            "\u001b[36m(_ray_fit pid=26693)\u001b[0m \tRan out of time, early stopping on iteration 2079.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.5539\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t207.46s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.3s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 122.87s of the 420.91s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m /usr/local/lib/python3.11/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.8488\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t21.86s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t1.52s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 95.07s of the 393.12s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.25%)\n",
            "\u001b[36m(_ray_fit pid=27342)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 6)\n",
            "\u001b[36m(_ray_fit pid=27595)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 7)\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=27864)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 7)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=28122)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 7)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.7391\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t120.3s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.82s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the 267.47s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tEnsemble Weights: {'CatBoost_BAG_L1': 0.727, 'LightGBM_BAG_L1': 0.136, 'LightGBMXT_BAG_L1': 0.045, 'ExtraTreesMSE_BAG_L1': 0.045, 'NeuralNetFastAI_BAG_L1': 0.045}\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.5443\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.12s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting 106 L2 models, fit_strategy=\"sequential\" ...\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 267.34s of the 267.29s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.28%)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=28399)\u001b[0m [1000]\tvalid_set's rmse: 1.29774\n",
            "\u001b[36m(_ray_fit pid=28398)\u001b[0m [2000]\tvalid_set's rmse: 1.76656\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(_ray_fit pid=28398)\u001b[0m [3000]\tvalid_set's rmse: 1.76292\n",
            "\u001b[36m(_ray_fit pid=28398)\u001b[0m [4000]\tvalid_set's rmse: 1.76083\n",
            "\u001b[36m(_ray_fit pid=28398)\u001b[0m [5000]\tvalid_set's rmse: 1.7603\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=28398)\u001b[0m \tRan out of time, early stopping on iteration 5945. Best iteration is:\n",
            "\u001b[36m(_ray_fit pid=28398)\u001b[0m \t[5706]\tvalid_set's rmse: 1.75855\n",
            "\u001b[36m(_ray_fit pid=28124)\u001b[0m \tRan out of time, stopping training early. (Stopping on epoch 7)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.6093\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t77.18s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t7.06s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: LightGBM_BAG_L2 ... Training model for up to 185.54s of the 185.50s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.27%)\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[36m(_ray_fit pid=29151)\u001b[0m [1000]\tvalid_set's rmse: 1.36773\n",
            "\u001b[36m(_ray_fit pid=29151)\u001b[0m [2000]\tvalid_set's rmse: 1.3544\n",
            "\u001b[36m(_ray_fit pid=29151)\u001b[0m [3000]\tvalid_set's rmse: 1.35119\n",
            "\u001b[36m(_ray_fit pid=29151)\u001b[0m [4000]\tvalid_set's rmse: 1.34955\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(_ray_fit pid=29151)\u001b[0m \tRan out of time, early stopping on iteration 4039. Best iteration is:\n",
            "\u001b[36m(_ray_fit pid=29151)\u001b[0m \t[3978]\tvalid_set's rmse: 1.34947\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.5878\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t70.1s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t2.95s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 111.71s of the 111.66s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m /usr/local/lib/python3.11/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m   warnings.warn(\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.5254\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t204.18s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t1.51s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.00s of the -96.45s of remaining time.\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \tEnsemble Weights: {'RandomForestMSE_BAG_L2': 0.474, 'CatBoost_BAG_L1': 0.263, 'LightGBM_BAG_L2': 0.158, 'LightGBMXT_BAG_L2': 0.105}\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t-1.4966\t = Validation score   (-root_mean_squared_error)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.11s\t = Training   runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m \t0.0s\t = Validation runtime\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m AutoGluon training complete, total runtime = 990.58s ... Best model: WeightedEnsemble_L3 | Estimated inference throughput: 210.6 rows/s (3597 batch size)\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels/firecast_prediction/ds_sub_fit/sub_fit_ho\")\n",
            "\u001b[36m(_dystack pid=23375)\u001b[0m Deleting DyStack predictor artifacts (clean_up_fits=True) ...\n",
            "Leaderboard on holdout data (DyStack):\n",
            "                     model  score_holdout  score_val              eval_metric  pred_time_test  pred_time_val    fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
            "0          CatBoost_BAG_L1      -1.381211  -1.553913  root_mean_squared_error        1.188541       0.303656  207.457704                 1.188541                0.303656         207.457704            1       True          6\n",
            "1      WeightedEnsemble_L2      -1.384690  -1.544254  root_mean_squared_error       10.591847       7.852932  470.369803                 0.003045                0.001437           0.116370            2       True          9\n",
            "2      WeightedEnsemble_L3      -1.404894  -1.496550  root_mean_squared_error       20.312332      22.233035  939.717824                 0.003355                0.001362           0.109350            3       True         13\n",
            "3   RandomForestMSE_BAG_L2      -1.433832  -1.525418  root_mean_squared_error       13.394908      12.223256  792.328748                 0.720796                1.506001         204.182188            2       True         12\n",
            "4          LightGBM_BAG_L1      -1.445876  -1.640010  root_mean_squared_error        1.014142       1.734343   62.859817                 1.014142                1.734343          62.859817            1       True          4\n",
            "5   NeuralNetFastAI_BAG_L1      -1.467473  -1.739126  root_mean_squared_error        1.049340       0.816521  120.304896                 1.049340                0.816521         120.304896            1       True          8\n",
            "6          LightGBM_BAG_L2      -1.474995  -1.587765  root_mean_squared_error       14.936219      13.668626  658.250448                 2.262108                2.951371          70.103888            2       True         11\n",
            "7        LightGBMXT_BAG_L1      -1.516398  -1.650779  root_mean_squared_error        5.161686       3.476592   57.775923                 5.161686                3.476592          57.775923            1       True          3\n",
            "8        LightGBMXT_BAG_L2      -1.530498  -1.609336  root_mean_squared_error       17.326073      17.774302  665.322399                 4.651961                7.057046          77.175839            2       True         10\n",
            "9     ExtraTreesMSE_BAG_L1      -1.697168  -1.848751  root_mean_squared_error        2.175093       1.520384   21.855094                 2.175093                1.520384          21.855094            1       True          7\n",
            "10  RandomForestMSE_BAG_L1      -1.769063  -1.874637  root_mean_squared_error        1.902632       1.648408  117.764612                 1.902632                1.648408         117.764612            1       True          5\n",
            "11   KNeighborsUnif_BAG_L1      -1.950035  -2.004267  root_mean_squared_error        0.095238       0.627211    0.067993                 0.095238                0.627211           0.067993            1       True          1\n",
            "12   KNeighborsDist_BAG_L1      -1.966345  -2.024835  root_mean_squared_error        0.087439       0.590140    0.060521                 0.087439                0.590140           0.060521            1       True          2\n",
            "\t0\t = Optimal   num_stack_levels (Stacked Overfitting Occurred: True)\n",
            "\t1020s\t = DyStack   runtime |\t2580s\t = Remaining runtime\n",
            "Starting main fit with num_stack_levels=0.\n",
            "\tFor future fit calls on this dataset, you can skip DyStack to save time: `predictor.fit(..., dynamic_stacking=False, num_stack_levels=0)`\n",
            "Beginning AutoGluon training ... Time limit = 2580s\n",
            "AutoGluon will save models to \"/content/AutogluonModels/firecast_prediction\"\n",
            "Train Data Rows:    32372\n",
            "Train Data Columns: 11\n",
            "Label Column:       call119_train.call_count\n",
            "Problem Type:       regression\n",
            "Preprocessing data ...\n",
            "Using Feature Generators to preprocess the data ...\n",
            "Fitting AutoMLPipelineFeatureGenerator...\n",
            "\tAvailable Memory:                    10153.25 MB\n",
            "\tTrain Data (Original)  Memory Usage: 8.26 MB (0.1% of available memory)\n",
            "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
            "\tStage 1 Generators:\n",
            "\t\tFitting AsTypeFeatureGenerator...\n",
            "\tStage 2 Generators:\n",
            "\t\tFitting FillNaFeatureGenerator...\n",
            "\tStage 3 Generators:\n",
            "\t\tFitting IdentityFeatureGenerator...\n",
            "\t\tFitting CategoryFeatureGenerator...\n",
            "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
            "\tStage 4 Generators:\n",
            "\t\tFitting DropUniqueFeatureGenerator...\n",
            "\tStage 5 Generators:\n",
            "\t\tFitting DropDuplicatesFeatureGenerator...\n",
            "\tTypes of features in original data (raw dtype, special dtypes):\n",
            "\t\t('float', [])  : 8 | ['call119_train.ta_max', 'call119_train.ta_min', 'call119_train.ta_max_min', 'call119_train.hm_min', 'call119_train.hm_max', ...]\n",
            "\t\t('int', [])    : 1 | ['year']\n",
            "\t\t('object', []) : 2 | ['call119_train.address_gu', 'call119_train.sub_address']\n",
            "\tTypes of features in processed data (raw dtype, special dtypes):\n",
            "\t\t('category', []) : 2 | ['call119_train.address_gu', 'call119_train.sub_address']\n",
            "\t\t('float', [])    : 8 | ['call119_train.ta_max', 'call119_train.ta_min', 'call119_train.ta_max_min', 'call119_train.hm_min', 'call119_train.hm_max', ...]\n",
            "\t\t('int', [])      : 1 | ['year']\n",
            "\t0.7s = Fit runtime\n",
            "\t11 features in original data used to generate 11 features in processed data.\n",
            "\tTrain Data (Processed) Memory Usage: 2.32 MB (0.0% of available memory)\n",
            "Data preprocessing and feature engineering runtime = 0.73s ...\n",
            "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
            "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
            "\tTo change this, specify the eval_metric parameter of Predictor()\n",
            "Large model count detected (112 configs) ... Only displaying the first 3 models of each family. To see all, set `verbosity=3`.\n",
            "User-specified model hyperparameters to be fit:\n",
            "{\n",
            "\t'NN_TORCH': [{}, {'activation': 'elu', 'dropout_prob': 0.10077639529843717, 'hidden_size': 108, 'learning_rate': 0.002735937344002146, 'num_layers': 4, 'use_batchnorm': True, 'weight_decay': 1.356433327634438e-12, 'ag_args': {'name_suffix': '_r79', 'priority': -2}}, {'activation': 'elu', 'dropout_prob': 0.11897478034205347, 'hidden_size': 213, 'learning_rate': 0.0010474382260641949, 'num_layers': 4, 'use_batchnorm': False, 'weight_decay': 5.594471067786272e-10, 'ag_args': {'name_suffix': '_r22', 'priority': -7}}],\n",
            "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, {'learning_rate': 0.03, 'num_leaves': 128, 'feature_fraction': 0.9, 'min_data_in_leaf': 3, 'ag_args': {'name_suffix': 'Large', 'priority': 0, 'hyperparameter_tune_kwargs': None}}],\n",
            "\t'CAT': [{}, {'depth': 6, 'grow_policy': 'SymmetricTree', 'l2_leaf_reg': 2.1542798306067823, 'learning_rate': 0.06864209415792857, 'max_ctr_complexity': 4, 'one_hot_max_size': 10, 'ag_args': {'name_suffix': '_r177', 'priority': -1}}, {'depth': 8, 'grow_policy': 'Depthwise', 'l2_leaf_reg': 2.7997999596449104, 'learning_rate': 0.031375015734637225, 'max_ctr_complexity': 2, 'one_hot_max_size': 3, 'ag_args': {'name_suffix': '_r9', 'priority': -5}}],\n",
            "\t'XGB': [{}, {'colsample_bytree': 0.6917311125174739, 'enable_categorical': False, 'learning_rate': 0.018063876087523967, 'max_depth': 10, 'min_child_weight': 0.6028633586934382, 'ag_args': {'name_suffix': '_r33', 'priority': -8}}, {'colsample_bytree': 0.6628423832084077, 'enable_categorical': False, 'learning_rate': 0.08775715546881824, 'max_depth': 5, 'min_child_weight': 0.6294123374222513, 'ag_args': {'name_suffix': '_r89', 'priority': -16}}],\n",
            "\t'FASTAI': [{}, {'bs': 256, 'emb_drop': 0.5411770367537934, 'epochs': 43, 'layers': [800, 400], 'lr': 0.01519848858318159, 'ps': 0.23782946566604385, 'ag_args': {'name_suffix': '_r191', 'priority': -4}}, {'bs': 2048, 'emb_drop': 0.05070411322605811, 'epochs': 29, 'layers': [200, 100], 'lr': 0.08974235041576624, 'ps': 0.10393466140748028, 'ag_args': {'name_suffix': '_r102', 'priority': -11}}],\n",
            "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
            "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
            "}\n",
            "Fitting 108 L1 models, fit_strategy=\"sequential\" ...\n",
            "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 2579.15s of the 2579.13s of remaining time.\n",
            "\t-2.0087\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.23s\t = Training   runtime\n",
            "\t1.1s\t = Validation runtime\n",
            "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 2573.23s of the 2573.20s of remaining time.\n",
            "\t-2.0223\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.17s\t = Training   runtime\n",
            "\t0.52s\t = Validation runtime\n",
            "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 2572.48s of the 2572.46s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.20%)\n",
            "\t-1.6349\t = Validation score   (-root_mean_squared_error)\n",
            "\t58.53s\t = Training   runtime\n",
            "\t6.8s\t = Validation runtime\n",
            "Fitting model: LightGBM_BAG_L1 ... Training model for up to 2508.28s of the 2508.25s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
            "\t-1.5828\t = Validation score   (-root_mean_squared_error)\n",
            "\t50.17s\t = Training   runtime\n",
            "\t1.98s\t = Validation runtime\n",
            "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 2453.09s of the 2453.06s of remaining time.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "\t-1.8437\t = Validation score   (-root_mean_squared_error)\n",
            "\t83.46s\t = Training   runtime\n",
            "\t2.15s\t = Validation runtime\n",
            "Fitting model: CatBoost_BAG_L1 ... Training model for up to 2365.48s of the 2365.45s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.97%)\n",
            "\t-1.5513\t = Validation score   (-root_mean_squared_error)\n",
            "\t348.05s\t = Training   runtime\n",
            "\t0.43s\t = Validation runtime\n",
            "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 2012.70s of the 2012.67s of remaining time.\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
            "  warnings.warn(\n",
            "\t-1.8237\t = Validation score   (-root_mean_squared_error)\n",
            "\t25.9s\t = Training   runtime\n",
            "\t1.65s\t = Validation runtime\n",
            "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 1978.95s of the 1978.92s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.27%)\n",
            "\t-1.5758\t = Validation score   (-root_mean_squared_error)\n",
            "\t345.22s\t = Training   runtime\n",
            "\t1.08s\t = Validation runtime\n",
            "Fitting model: XGBoost_BAG_L1 ... Training model for up to 1626.93s of the 1626.90s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.28%)\n",
            "\t-1.7582\t = Validation score   (-root_mean_squared_error)\n",
            "\t99.85s\t = Training   runtime\n",
            "\t0.78s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_BAG_L1 ... Training model for up to 1521.85s of the 1521.83s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.15%)\n",
            "\t-1.7605\t = Validation score   (-root_mean_squared_error)\n",
            "\t1114.35s\t = Training   runtime\n",
            "\t0.56s\t = Validation runtime\n",
            "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 403.31s of the 403.28s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.36%)\n",
            "\t-1.5555\t = Validation score   (-root_mean_squared_error)\n",
            "\t54.93s\t = Training   runtime\n",
            "\t3.45s\t = Validation runtime\n",
            "Fitting model: CatBoost_r177_BAG_L1 ... Training model for up to 341.21s of the 341.19s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=3.86%)\n",
            "\t-1.5368\t = Validation score   (-root_mean_squared_error)\n",
            "\t211.35s\t = Training   runtime\n",
            "\t0.36s\t = Validation runtime\n",
            "Fitting model: NeuralNetTorch_r79_BAG_L1 ... Training model for up to 122.82s of the 122.79s of remaining time.\n",
            "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (2 workers, per: cpus=1, gpus=0, memory=0.15%)\n",
            "\t-2.0876\t = Validation score   (-root_mean_squared_error)\n",
            "\t129.16s\t = Training   runtime\n",
            "\t0.69s\t = Validation runtime\n",
            "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.00s of the -12.76s of remaining time.\n",
            "\tEnsemble Weights: {'NeuralNetFastAI_BAG_L1': 0.409, 'CatBoost_r177_BAG_L1': 0.364, 'LightGBMLarge_BAG_L1': 0.182, 'ExtraTreesMSE_BAG_L1': 0.045}\n",
            "\t-1.4617\t = Validation score   (-root_mean_squared_error)\n",
            "\t0.25s\t = Training   runtime\n",
            "\t0.0s\t = Validation runtime\n",
            "AutoGluon training complete, total runtime = 2592.98s ... Best model: WeightedEnsemble_L2 | Estimated inference throughput: 795.4 rows/s (4047 batch size)\n",
            "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"/content/AutogluonModels/firecast_prediction\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Making predictions on the 2023 test data...\n",
            "\n",
            "Calculated RMSE on 2023 test data: 1.1252\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error # For manual RMSE calculation\n",
        "from autogluon.tabular import TabularPredictor\n",
        "\n",
        "# Load the preprocessed dataset\n",
        "file_path = '/content/drive/MyDrive/AIVLE/contest_for_firecast/call119_train1.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.replace(-99, np.nan, inplace=True)\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df = df.drop(columns=['call119_train.address_city', 'call119_train.stn'])\n",
        "\n",
        "\n",
        "\n",
        "# Extract year from 'call119_train.tm'\n",
        "df['year'] = df['call119_train.tm'].astype(str).str[:4].astype(int)\n",
        "\n",
        "# Separate data into train and test based on year\n",
        "train_df = df[df['year'].isin([2020, 2021, 2022])].copy()\n",
        "test_df = df[df['year'] == 2023].copy()\n",
        "\n",
        "# Define features (X) and target (Y)\n",
        "# The 'Unnamed: 0' column is typically an index and should not be a feature.\n",
        "# The 'year' column was used for splitting and also shouldn't be a feature for the model.\n",
        "features = [col for col in df.columns if col not in ['Unnamed: 0', 'call119_train.call_count']]\n",
        "target = 'call119_train.call_count'\n",
        "\n",
        "# Prepare the training data for AutoGluon\n",
        "# AutoGluon's `fit` method expects a single DataFrame containing both features and the target.\n",
        "# We'll include the target column 'call119_train.call_count' in 'train_data_autogluon'.\n",
        "train_data_autogluon = train_df[features + [target]].drop('call119_train.tm', axis=1)\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the test data for AutoGluon prediction and evaluation\n",
        "# For prediction, you pass only the features.\n",
        "# For evaluation (using AutoGluon's .evaluate()), you pass features + target.\n",
        "test_data_features = test_df[features].drop('call119_train.tm', axis=1)\n",
        "Y_test_actual = test_df[target] # Store the actual target values for RMSE calculation\n",
        "\n",
        "# --- AutoGluon Model Training and Evaluation ---\n",
        "\n",
        "# Initialize the AutoGluon TabularPredictor\n",
        "# 'label' specifies the target column.\n",
        "# 'eval_metric' tells AutoGluon what metric to optimize for and report.\n",
        "# 'path' defines where AutoGluon will save its models and logs.\n",
        "predictor = TabularPredictor(\n",
        "    label=target,\n",
        "    eval_metric='root_mean_squared_error', # Set evaluation metric to RMSE\n",
        "    problem_type='regression',\n",
        "    path='AutogluonModels/firecast_prediction' # Directory to save model artifacts\n",
        ")\n",
        "\n",
        "# Train the predictor\n",
        "# We pass the combined 'train_data_autogluon' which includes both features and the target.\n",
        "# 'presets='best_quality'' will train a variety of models for optimal performance.\n",
        "print(\"\\nStarting AutoGluon training...\")\n",
        "predictor.fit(\n",
        "    train_data=train_data_autogluon,\n",
        "    presets='best_quality',\n",
        "    time_limit=3600 # Optional: set a time limit for training in seconds (e.g., 1 hour)\n",
        ")\n",
        "\n",
        "# Make predictions on the test set features (X_test equivalent)\n",
        "print(\"\\nMaking predictions on the 2023 test data...\")\n",
        "predictions = predictor.predict(test_data_features)\n",
        "\n",
        "# Calculate RMSE manually using the predictions and the actual Y_test values\n",
        "rmse_manual = np.sqrt(mean_squared_error(Y_test_actual, predictions))\n",
        "print(f\"\\nCalculated RMSE on 2023 test data: {rmse_manual:.4f}\")\n",
        "\n",
        "# Optional: Use AutoGluon's built-in evaluate method for a more comprehensive report\n",
        "# For this, you need to provide a DataFrame with both features and the actual target.\n",
        "# test_data_for_autogluon_eval = pd.concat([test_data_features, Y_test_actual], axis=1)\n",
        "# evaluation_results = predictor.evaluate(test_data_for_autogluon_eval, silent=True)\n",
        "# print(\"\\nAutoGluon's detailed evaluation on 2023 test data:\")\n",
        "# print(f\"Overall Test Performance (RMSE from AutoGluon): {evaluation_results['root_mean_squared_error']:.4f}\")\n",
        "\n",
        "# print(\"\\nAutoGluon process complete!\")\n",
        "# print(f\"First 5 predictions:\\n{predictions.head()}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOeU1erOkCMQGFJqP0aj8kh",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}